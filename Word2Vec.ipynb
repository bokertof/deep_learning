{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmAua2LLD8PU",
        "colab_type": "text"
      },
      "source": [
        "# Importing numpy lib and text-sample for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AujmyH2RAOnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "text = '''Chemical autoencoders are attractive models as they combine chemical space navigation with possibilities for de novo molecule generation in areas of interest.\n",
        " This enables them to produce focused chemical libraries around a single lead compound for employment early in a drug discovery project. Here, it is shown that the choice\n",
        "  of chemical representation, such as strings from the simplified molecular-input line-entry system (SMILES), has a large influence on the properties of the latent space.\n",
        "   It is further explored to what extent translating between different chemical representations influences the latent space similarity to the SMILES strings or circular fingerprints.\n",
        "    By employing SMILES enumeration for either the encoder or decoder, it is found that the decoder has the largest influence on the properties of the latent space.\n",
        "     Training a sequence to sequence heteroencoder based on recurrent neural networks (RNNs) with long short-term memory cells (LSTM) to predict different enumerated SMILES strings\n",
        "      from the same canonical SMILES string gives the largest similarity between latent space distance and molecular similarity measured as circular fingerprints similarity.\n",
        "       Using the output from the code layer in quantitative structure activity relationship (QSAR) of five molecular datasets shows that heteroencoder derived vectors markedly\n",
        "        outperforms autoencoder derived vectors as well as models built using ECFP4 fingerprints, underlining the increased chemical relevance of the latent space.\n",
        "         However, the use of enumeration during training of the decoder leads to a marked increase in the rate of decoding to different molecules than encoded,\n",
        "          a tendency that can be counteracted with more complex network architectures.'''\n",
        "\n",
        "## text is from doi:10.3390/biom8040131"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1SEiFBgCCew",
        "colab_type": "text"
      },
      "source": [
        "# Some helper-function and generator for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am4SxFaeAaSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "\n",
        "    bad_symb = '''.,'\"{}[]()1234567890!?^:;#@%$~`*><'''\n",
        "    for i in bad_symb:\n",
        "        text = text.replace(i,'')\n",
        "\n",
        "    words = [word.lower() for word in text.split()]\n",
        "    vocab = dict([(c,i) for i,c in enumerate(sorted(set(words)))])\n",
        "    print('Preprocessing was finished, bad symbols were deleted, the text was lowercased and the dictionary was made')\n",
        "    print('Number of words in dictionary: ', len(vocab))\n",
        "\n",
        "    return words, vocab\n",
        "    \n",
        "\n",
        "def generator(words, vocab, window_size, cbow = True):\n",
        "\n",
        "    vocab_len = len(vocab)\n",
        "    one_hot = np.eye(vocab_len)\n",
        "    \n",
        "    for i in range(len(words)):\n",
        "        index = i-window_size\n",
        "        if index < 0:\n",
        "            index = 0\n",
        "        context = words[index:i] + words[i+1:i+window_size+1]\n",
        "\n",
        "        if not cbow:\n",
        "            yield (one_hot[vocab[words[i]]], vocab[words[i]]), one_hot[[vocab[i] for i in context]]\n",
        "        else:\n",
        "            yield one_hot[vocab[words[i]]], one_hot[[vocab[i] for i in context]]\n",
        "\n",
        "\n",
        "\n",
        "def softmax(vector):    \n",
        "    return  np.exp(vector) / np.sum(np.exp(vector), axis = 1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBo2N2_QC7d5",
        "colab_type": "text"
      },
      "source": [
        "# **The main two classes - implementations of CBOW and SkipGram algorithms.** Note that these implementations are not optimized, i.e. without Negative Sampling, Hierarchical Softmax etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUAm1Mj0Ae0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBOW():\n",
        "    \n",
        "    def __init__(self, vocab_size, embedd_size, rate):\n",
        "        \n",
        "        self.embedd_size = embedd_size\n",
        "        self.W1 = np.random.uniform(-0.8, 0.8, (vocab_size, embedd_size))\n",
        "        self.W2 = np.random.uniform(-0.8, 0.8, (embedd_size, vocab_size))\n",
        "        self.rate = rate\n",
        "        \n",
        "                 \n",
        "    def forward_pass(self, context):\n",
        "\n",
        "        self.aver_x = np.mean(context, axis = 0)\n",
        "        \n",
        "        self.H = np.dot(self.aver_x, self.W1).reshape(-1,self.embedd_size)\n",
        "        self.O = np.dot(self.H, self.W2)\n",
        "\n",
        "        return softmax(self.O)\n",
        "\n",
        "    \n",
        "    def get_loss(self, word):\n",
        "        \n",
        "        return -self.O[0][np.argwhere(word == 1)].item() + np.log(np.sum(np.exp(self.O)))\n",
        "\n",
        "    def backward(self, out, word, k = 10):\n",
        "\n",
        "        error = out - word\n",
        "\n",
        "        dw2 = np.outer(self.H, error)\n",
        "        dw1 = np.outer(self.aver_x, np.dot(self.W2, error.T))\n",
        "\n",
        "        \n",
        "        self.W2 = self.W2 - self.rate * dw2\n",
        "        self.W1 = self.W1 - self.rate * dw1\n",
        "\n",
        "    def get_embedd(self):\n",
        "\n",
        "        return self.W2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SKIPGRAM():\n",
        "    \n",
        "    def __init__(self, vocab_size, embedd_size, rate):\n",
        "        \n",
        "        self.embedd_size = embedd_size\n",
        "        self.W1 = np.random.uniform(-0.8, 0.8, (vocab_size, embedd_size))\n",
        "        self.W2 = np.random.uniform(-0.8, 0.8, (embedd_size, vocab_size))\n",
        "        self.rate = rate\n",
        "        \n",
        "                 \n",
        "    def forward_pass(self, word, idx):\n",
        "\n",
        "        self.word = word\n",
        "        self.idx = idx\n",
        "\n",
        "        self.H = self.W1[self.idx,:].reshape(-1,1)\n",
        "        self.O = np.dot(self.H.T, self.W2)\n",
        "\n",
        "        return softmax(self.O)\n",
        "\n",
        "    \n",
        "    def get_loss(self, context):\n",
        "\n",
        "        return -np.sum([self.O[0][np.argwhere(word == 1)] for word in context]) + context.shape[0]*np.log(np.sum(np.exp(self.O)))\n",
        "\n",
        "    def backward(self, out, context):\n",
        "\n",
        "\n",
        "        EI = np.sum([np.subtract(out, word) for word in context], axis=0)\n",
        "\n",
        "        dW2 = np.outer(self.H, EI)\n",
        "        dW1 = np.outer(self.word, np.dot(self.W2, EI.T))\n",
        "\n",
        "        self.W2 = self.W2 - self.rate * dW2\n",
        "        self.W1 = self.W1 - self.rate * dW1\n",
        "\n",
        "    def get_embedd(self):\n",
        "\n",
        "        return self.W1           "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWMh4Ed-EZS5",
        "colab_type": "text"
      },
      "source": [
        "# **Function for training process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJBh1XeLAjUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(data, cbow = True, epochs = 500, window_size = 1, embedd_size = 1, rate = 0.01):\n",
        "    \n",
        "    words, vocab = preprocess(data)\n",
        "\n",
        "    if cbow:\n",
        "        print(\"Chosen algorithm: CBOW\")\n",
        "        model = CBOW(len(vocab), embedd_size, rate)\n",
        "    else:\n",
        "        print(\"Chosen algorithm: SkipGram\")\n",
        "        model = SKIPGRAM(len(vocab), embedd_size, rate)\n",
        "    \n",
        "    \n",
        "    for epoch in range(1, epochs+1):\n",
        "        sum_loss = 0\n",
        "        gener = generator(words, vocab, window_size, cbow = cbow)\n",
        "        \n",
        "        for word, context in gener:\n",
        "            \n",
        "\n",
        "            if cbow:\n",
        "                out = model.forward_pass(context)\n",
        "                sum_loss += model.get_loss(word)\n",
        "\n",
        "                model.backward(out, word)\n",
        "            else:\n",
        "                out = model.forward_pass(word[0], word[1])\n",
        "                sum_loss += model.get_loss(context)\n",
        "                \n",
        "                model.backward(out, context)\n",
        "\n",
        "        print(f\"| EPOCH № {epoch :3.0f} | LOSS {sum_loss / len(vocab):.5f} |\")\n",
        "            \n",
        "    embeddings = model.get_embedd()\n",
        "\n",
        "    if cbow:\n",
        "        n = 0\n",
        "        for key, value in vocab.items():\n",
        "            vocab[key] = embeddings[:,n]\n",
        "            n += 1\n",
        "    else:\n",
        "        n = 0\n",
        "        for key, value in vocab.items():\n",
        "            vocab[key] = embeddings[n,:]\n",
        "            n += 1    \n",
        "    return vocab"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXPFb8vtAlkb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "ba567626-69ec-405a-b838-9e68a6a54a04"
      },
      "source": [
        "embedd_matrix = train(text, cbow = False, epochs = 20, window_size = 4, embedd_size = 3, rate = 0.01)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing was finished, bad symbols were deleted, the text was lowercased and the dictionary was made\n",
            "Number of words in dictionary:  146\n",
            "Chosen algorithm: SkipGram\n",
            "| EPOCH №   1 | LOSS 68.92754 |\n",
            "| EPOCH №   2 | LOSS 68.55180 |\n",
            "| EPOCH №   3 | LOSS 68.29436 |\n",
            "| EPOCH №   4 | LOSS 68.09626 |\n",
            "| EPOCH №   5 | LOSS 67.92907 |\n",
            "| EPOCH №   6 | LOSS 67.77580 |\n",
            "| EPOCH №   7 | LOSS 67.62317 |\n",
            "| EPOCH №   8 | LOSS 67.45744 |\n",
            "| EPOCH №   9 | LOSS 67.26159 |\n",
            "| EPOCH №  10 | LOSS 67.01307 |\n",
            "| EPOCH №  11 | LOSS 66.68449 |\n",
            "| EPOCH №  12 | LOSS 66.25501 |\n",
            "| EPOCH №  13 | LOSS 65.74275 |\n",
            "| EPOCH №  14 | LOSS 65.21697 |\n",
            "| EPOCH №  15 | LOSS 64.72383 |\n",
            "| EPOCH №  16 | LOSS 64.25876 |\n",
            "| EPOCH №  17 | LOSS 63.81201 |\n",
            "| EPOCH №  18 | LOSS 63.38038 |\n",
            "| EPOCH №  19 | LOSS 62.96377 |\n",
            "| EPOCH №  20 | LOSS 62.56252 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cknSdPCwA5pk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e778d83-9658-427f-9da9-b6fcbe4196a7"
      },
      "source": [
        "print('{0:<30} {1}'.format('\\033[1m'+'WORD:', '\\033[1m'+'ITS EMBEDDING:'+'\\033[0m'))\n",
        "\n",
        "for key, value in embedd_matrix.items():\n",
        "    print('{0:<15} {1}'.format(key, value))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mWORD:                      \u001b[1mITS EMBEDDING:\u001b[0m\n",
            "a               [-0.71950402  0.19014475  0.35748951]\n",
            "activity        [-0.77394885  0.69306238 -0.75172485]\n",
            "and             [-0.21999558 -0.17156522 -0.13102351]\n",
            "architectures   [ 0.85103151 -0.27673316 -0.59617004]\n",
            "are             [-0.34195148  0.35540273  0.15552566]\n",
            "areas           [-0.6446661   1.01901023  0.58281092]\n",
            "around          [ 0.27784189 -0.42979749 -0.76278331]\n",
            "as              [-0.84488166  0.46487471 -0.27227158]\n",
            "attractive      [-0.11213078 -0.40852608  0.41413694]\n",
            "autoencoder     [0.04143346 0.10921397 0.66711346]\n",
            "autoencoders    [-0.09982664  0.40762047 -0.85332113]\n",
            "based           [0.04207131 0.17024031 0.50096703]\n",
            "be              [ 0.73039243 -0.18719958  0.24761619]\n",
            "between         [0.26107065 0.30192845 0.49008755]\n",
            "built           [-0.71362189  0.08054468 -1.07995724]\n",
            "by              [0.14222712 0.05850637 0.45264653]\n",
            "can             [ 0.21010938 -0.53165574 -0.40563127]\n",
            "canonical       [-0.20661727 -1.03334686  0.53015043]\n",
            "cells           [ 0.27754638 -0.22736149  0.36556001]\n",
            "chemical        [-0.75482785 -0.6127067  -0.20432089]\n",
            "choice          [-0.8133831  -0.41873893 -0.31817077]\n",
            "circular        [-0.54340168 -0.7867348  -0.01954338]\n",
            "code            [-0.36102321 -0.15828334 -0.08558832]\n",
            "combine         [-0.45895474  0.73238536 -0.06945157]\n",
            "complex         [ 0.59652236 -0.76349564 -0.82809355]\n",
            "compound        [-0.3718018   0.22287717  0.63307202]\n",
            "counteracted    [ 0.22694093  0.38487093 -0.81795321]\n",
            "datasets        [ 0.26401003 -0.07578491 -0.25800044]\n",
            "de              [ 0.70709223  0.16609594 -0.50272639]\n",
            "decoder         [-0.35475869 -1.10475492  0.85846181]\n",
            "decoding        [-1.14218889  0.68988955  0.984647  ]\n",
            "derived         [-0.05323744 -0.04051876  0.36555603]\n",
            "different       [-0.43242753 -0.26950051  0.2144727 ]\n",
            "discovery       [-0.72774844  0.25655706  0.54784646]\n",
            "distance        [ 0.01072193 -0.11228597 -0.37378954]\n",
            "drug            [0.05105142 0.48114928 0.26696078]\n",
            "during          [-0.98355493 -0.03307467  0.81969211]\n",
            "early           [-0.56534311  0.08561776  0.8392633 ]\n",
            "ecfp            [-0.63516862 -0.7798161  -0.87343416]\n",
            "either          [ 0.46378086 -0.96519577  0.12323045]\n",
            "employing       [-0.36327376 -0.30234067 -0.16109672]\n",
            "employment      [0.07113262 0.11865401 0.67163729]\n",
            "enables         [-0.34787353  0.58028481  0.3041996 ]\n",
            "encoded         [ 0.04448039 -0.3387328  -0.16766279]\n",
            "encoder         [-0.86460308 -0.48159363 -0.06480619]\n",
            "enumerated      [-0.2264808  -0.43311795  0.31410347]\n",
            "enumeration     [-0.31066877 -1.19256278  0.60283196]\n",
            "explored        [-0.68467249  0.36281519  0.4566609 ]\n",
            "extent          [-0.25689116  0.62730825  0.77064704]\n",
            "fingerprints    [-0.72656613  0.03529183  0.03912413]\n",
            "five            [-0.57570995  0.83355527  0.00348836]\n",
            "focused         [ 0.11673427  0.25591627 -0.41265913]\n",
            "for             [ 0.11534549 -0.71844811 -0.11093879]\n",
            "found           [-0.74972766 -0.77721661  0.33603117]\n",
            "from            [-0.69096209 -0.35222209  0.64202984]\n",
            "further         [-0.51305387 -0.18743521  0.37353617]\n",
            "generation      [ 0.25371509 -0.3271574   0.20436797]\n",
            "gives           [-0.43280082 -0.05642678  0.88743356]\n",
            "has             [-0.17258175 -1.04701462  0.31453913]\n",
            "here            [-0.53418869  0.28330002  0.62253543]\n",
            "heteroencoder   [ 0.6020188  -0.12791538  0.25529903]\n",
            "however         [-0.16590553  0.28329056  1.02385224]\n",
            "in              [-0.61700018  0.02551132  0.61402787]\n",
            "increase        [-0.96782498  0.42691654  0.8092548 ]\n",
            "increased       [-0.8184729  -0.8123488  -0.10388218]\n",
            "influence       [-0.54397644 -0.43988316  1.04572484]\n",
            "influences      [-0.60336262 -0.10036878  0.450079  ]\n",
            "interest        [0.31405504 0.87781128 1.06642113]\n",
            "is              [-0.95944465  0.11383338  0.73530344]\n",
            "it              [-1.14979673  0.10851733  0.60954295]\n",
            "large           [-0.36327353 -0.84545627  0.01853175]\n",
            "largest         [-0.65345989 -0.68921978  0.47368591]\n",
            "latent          [-1.29895419  0.17068122  0.50125507]\n",
            "layer           [-0.10335674 -0.23910553  0.4867943 ]\n",
            "lead            [-0.08429771 -0.07344042 -0.22499661]\n",
            "leads           [-1.00958164  0.10641236 -0.22507873]\n",
            "libraries       [ 0.02021268  0.09020547 -0.68880745]\n",
            "line-entry      [-0.07181183 -0.9458302  -0.41862942]\n",
            "long            [-0.19007718  0.7747953  -0.60885777]\n",
            "lstm            [0.23194343 0.13634643 0.597458  ]\n",
            "marked          [-0.15223612 -0.56982309 -0.31371812]\n",
            "markedly        [ 0.5687601  -0.36510725 -0.0350593 ]\n",
            "measured        [-0.48971227 -0.0594656  -0.06482321]\n",
            "memory          [-0.43458918  0.14621307 -0.28596654]\n",
            "models          [-0.74402722 -0.24424249 -0.61145453]\n",
            "molecular       [0.09632413 0.17676351 0.17448602]\n",
            "molecular-input [ 0.39396463 -1.0387153   0.86679703]\n",
            "molecule        [ 0.39552066  0.06859451 -0.14439408]\n",
            "molecules       [0.11058703 0.43068963 0.16024597]\n",
            "more            [ 0.07657491 -0.32297184 -0.83596631]\n",
            "navigation      [-0.17279434  0.52138996 -0.24450935]\n",
            "network         [ 0.74119958 -0.09555655  0.47756522]\n",
            "networks        [0.73427624 0.07381757 0.22497652]\n",
            "neural          [0.4831978  0.34656602 0.35912688]\n",
            "novo            [0.62017642 0.06668008 0.08630175]\n",
            "of              [-1.15849773  0.07192843  0.15187892]\n",
            "on              [-0.72381488 -0.05511283  0.94997735]\n",
            "or              [-0.07889153 -0.73858484  0.60661146]\n",
            "outperforms     [-0.24818895  0.52979714 -0.31412276]\n",
            "output          [-1.22382798 -0.16956019 -0.18940011]\n",
            "possibilities   [-0.0816449   0.67536048 -0.38162671]\n",
            "predict         [-0.2721024   0.39618673  0.54823085]\n",
            "produce         [-0.34857382  1.16673729  0.25561493]\n",
            "project         [-0.89285817  0.22070945 -0.42880052]\n",
            "properties      [-1.12484367 -0.35428457  0.22165366]\n",
            "qsar            [-0.90250518  0.97065023 -0.16911073]\n",
            "quantitative    [-0.72224544 -0.61982002 -0.04741547]\n",
            "rate            [-0.05406859  0.08813926  0.62864948]\n",
            "recurrent       [0.64887566 0.03143459 0.51482071]\n",
            "relationship    [-0.60401739  0.0304483  -0.40017417]\n",
            "relevance       [-0.43737353  0.08017397 -0.22723574]\n",
            "representation  [-0.50294574 -0.4627157   0.56843598]\n",
            "representations [ 0.11906781 -0.3964937   0.34827832]\n",
            "rnns            [-0.16998507  0.59451729  0.27720618]\n",
            "same            [-0.13505045 -0.68946097 -0.16933282]\n",
            "sequence        [ 0.66055661  0.02720513 -0.12107149]\n",
            "short-term      [-0.4689805   0.96356323 -0.40172856]\n",
            "shown           [-0.17426314 -0.26503681  0.72259082]\n",
            "shows           [0.04573715 0.95076525 0.2264729 ]\n",
            "similarity      [-0.46784441 -0.67874747  0.47472578]\n",
            "simplified      [-0.68091496 -0.32583037 -0.10562808]\n",
            "single          [0.23472724 0.38186728 0.43836205]\n",
            "smiles          [-0.14454252 -0.74720301  1.29967717]\n",
            "space           [-0.6105002  -0.68187043  0.57945655]\n",
            "string          [-0.72584546 -0.11679887  0.38046991]\n",
            "strings         [-0.77381013 -0.24834675  0.59289727]\n",
            "structure       [-0.17563104  0.49036138 -0.20421323]\n",
            "such            [-0.83395705 -0.21015864  0.16001458]\n",
            "system          [-0.03962457 -0.95924438 -0.10829918]\n",
            "tendency        [-0.33728121  0.33794092 -0.14168695]\n",
            "than            [-0.23451896 -0.4754185  -0.21437376]\n",
            "that            [-0.57913313 -0.0749722   0.1526321 ]\n",
            "the             [-0.47991818 -0.71581973  0.57156733]\n",
            "them            [-0.18674026  0.65554471  0.2069451 ]\n",
            "they            [-0.69421306  0.65965266 -0.26175689]\n",
            "this            [-0.31154625  0.93685443  0.13206842]\n",
            "to              [-0.72628722  0.00548468  0.43884927]\n",
            "training        [-1.14327581  0.03960203  0.2941944 ]\n",
            "translating     [-0.45815471  0.49043461  0.40473529]\n",
            "underlining     [-0.21243323 -0.17317875 -0.19304235]\n",
            "use             [-0.52245626 -0.10455235 -0.51345839]\n",
            "using           [-1.16097171  0.12962043  0.34226333]\n",
            "vectors         [-0.37487697 -0.3312335  -0.34085044]\n",
            "well            [-0.68970772 -0.22531874 -0.15089865]\n",
            "what            [-0.14990566 -0.03351604  0.01165226]\n",
            "with            [ 0.09945759  0.82209505 -0.15353321]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}